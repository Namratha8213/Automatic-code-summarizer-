<<<<<<< HEAD
{"cells":[{"cell_type":"markdown","id":"5b7c93ca","metadata":{},"source":["# GPT2 MODEL"]},{"cell_type":"markdown","id":"e467c2fd","metadata":{},"source":["Installing the dependencies"]},{"cell_type":"code","execution_count":null,"id":"3b3578df-f132-4673-9f93-6f9df056952c","metadata":{"id":"3b3578df-f132-4673-9f93-6f9df056952c","outputId":"287e92ee-d6aa-4bc1-c60b-e4936b97fe5f","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in d:\\acs\\myenv\\lib\\site-packages (4.42.3)\n","Requirement already satisfied: torch in d:\\acs\\myenv\\lib\\site-packages (2.3.1)\n","Requirement already satisfied: pandas in d:\\acs\\myenv\\lib\\site-packages (2.0.3)\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n","Requirement already satisfied: datasets in d:\\acs\\myenv\\lib\\site-packages (2.20.0)\n","Requirement already satisfied: accelerate in d:\\acs\\myenv\\lib\\site-packages (0.32.1)\n","Requirement already satisfied: evaluate in d:\\acs\\myenv\\lib\\site-packages (0.4.2)\n","Requirement already satisfied: filelock in d:\\acs\\myenv\\lib\\site-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\acs\\myenv\\lib\\site-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy<2.0,>=1.17 in d:\\acs\\myenv\\lib\\site-packages (from transformers) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in d:\\acs\\myenv\\lib\\site-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in d:\\acs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in d:\\acs\\myenv\\lib\\site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in d:\\acs\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in d:\\acs\\myenv\\lib\\site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in d:\\acs\\myenv\\lib\\site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in d:\\acs\\myenv\\lib\\site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in d:\\acs\\myenv\\lib\\site-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in d:\\acs\\myenv\\lib\\site-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in d:\\acs\\myenv\\lib\\site-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in d:\\acs\\myenv\\lib\\site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in d:\\acs\\myenv\\lib\\site-packages (from torch) (2024.5.0)\n","Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\acs\\myenv\\lib\\site-packages (from torch) (2021.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in d:\\acs\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in d:\\acs\\myenv\\lib\\site-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in d:\\acs\\myenv\\lib\\site-packages (from pandas) (2024.1)\n","Collecting scipy>=1.5.0 (from scikit-learn)\n","  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl.metadata (58 kB)\n","     ---------------------------------------- 0.0/59.0 kB ? eta -:--:--\n","     ---------------------------------- ----- 51.2/59.0 kB 1.3 MB/s eta 0:00:01\n","     ---------------------------------------- 59.0/59.0 kB 1.0 MB/s eta 0:00:00\n","Collecting joblib>=1.1.1 (from scikit-learn)\n","  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n","Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n","  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: pyarrow>=15.0.0 in d:\\acs\\myenv\\lib\\site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in d:\\acs\\myenv\\lib\\site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\acs\\myenv\\lib\\site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: xxhash in d:\\acs\\myenv\\lib\\site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in d:\\acs\\myenv\\lib\\site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in d:\\acs\\myenv\\lib\\site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: psutil in d:\\acs\\myenv\\lib\\site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in d:\\acs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in d:\\acs\\myenv\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in d:\\acs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in d:\\acs\\myenv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in d:\\acs\\myenv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\acs\\myenv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: intel-openmp==2021.* in d:\\acs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n","Requirement already satisfied: tbb==2021.* in d:\\acs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n","Requirement already satisfied: six>=1.5 in d:\\acs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in d:\\acs\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in d:\\acs\\myenv\\lib\\site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\acs\\myenv\\lib\\site-packages (from requests->transformers) (2.2.2)\n","Requirement already satisfied: certifi>=2017.4.17 in d:\\acs\\myenv\\lib\\site-packages (from requests->transformers) (2024.7.4)\n","Requirement already satisfied: colorama in d:\\acs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in d:\\acs\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in d:\\acs\\myenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n","Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n","   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n","   ---------------------------------------- 0.1/9.3 MB 2.6 MB/s eta 0:00:04\n","   -- ------------------------------------- 0.6/9.3 MB 6.0 MB/s eta 0:00:02\n","   ------- -------------------------------- 1.7/9.3 MB 11.9 MB/s eta 0:00:01\n","   --------- ------------------------------ 2.1/9.3 MB 12.2 MB/s eta 0:00:01\n","   --------- ------------------------------ 2.1/9.3 MB 11.4 MB/s eta 0:00:01\n","   ------------ --------------------------- 3.0/9.3 MB 11.2 MB/s eta 0:00:01\n","   ------------------ --------------------- 4.2/9.3 MB 12.7 MB/s eta 0:00:01\n","   ------------------ --------------------- 4.2/9.3 MB 12.7 MB/s eta 0:00:01\n","   --------------------- ------------------ 4.9/9.3 MB 12.5 MB/s eta 0:00:01\n","   --------------------- ------------------ 4.9/9.3 MB 10.8 MB/s eta 0:00:01\n","   --------------------- ------------------ 4.9/9.3 MB 10.5 MB/s eta 0:00:01\n","   ---------------------- ----------------- 5.1/9.3 MB 9.3 MB/s eta 0:00:01\n","   ----------------------- ---------------- 5.3/9.3 MB 9.2 MB/s eta 0:00:01\n","   ------------------------- -------------- 5.8/9.3 MB 8.8 MB/s eta 0:00:01\n","   ------------------------- -------------- 6.0/9.3 MB 8.7 MB/s eta 0:00:01\n","   -------------------------- ------------- 6.1/9.3 MB 8.3 MB/s eta 0:00:01\n","   --------------------------- ------------ 6.3/9.3 MB 8.1 MB/s eta 0:00:01\n","   ---------------------------- ----------- 6.5/9.3 MB 7.7 MB/s eta 0:00:01\n","   ----------------------------- ---------- 6.7/9.3 MB 7.4 MB/s eta 0:00:01\n","   ----------------------------- ---------- 6.8/9.3 MB 7.3 MB/s eta 0:00:01\n","   ------------------------------ --------- 7.0/9.3 MB 7.2 MB/s eta 0:00:01\n","   ------------------------------- -------- 7.2/9.3 MB 6.9 MB/s eta 0:00:01\n","   ------------------------------- -------- 7.4/9.3 MB 6.7 MB/s eta 0:00:01\n","   -------------------------------- ------- 7.5/9.3 MB 6.6 MB/s eta 0:00:01\n","   --------------------------------- ------ 7.7/9.3 MB 6.5 MB/s eta 0:00:01\n","   --------------------------------- ------ 7.8/9.3 MB 6.3 MB/s eta 0:00:01\n","   ---------------------------------- ----- 8.0/9.3 MB 6.3 MB/s eta 0:00:01\n","   ----------------------------------- ---- 8.3/9.3 MB 6.1 MB/s eta 0:00:01\n","   ------------------------------------ --- 8.4/9.3 MB 6.0 MB/s eta 0:00:01\n","   ------------------------------------- -- 8.6/9.3 MB 6.0 MB/s eta 0:00:01\n","   ------------------------------------- -- 8.7/9.3 MB 5.9 MB/s eta 0:00:01\n","   -------------------------------------- - 8.9/9.3 MB 5.8 MB/s eta 0:00:01\n","   ---------------------------------------  9.1/9.3 MB 5.8 MB/s eta 0:00:01\n","   ---------------------------------------  9.3/9.3 MB 5.6 MB/s eta 0:00:01\n","   ---------------------------------------- 9.3/9.3 MB 5.5 MB/s eta 0:00:00\n","Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n","   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n","   ----------------------- ---------------- 174.1/301.8 kB 5.3 MB/s eta 0:00:01\n","   ---------------------------------------  297.0/301.8 kB 3.7 MB/s eta 0:00:01\n","   ---------------------------------------- 301.8/301.8 kB 3.1 MB/s eta 0:00:00\n","Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n","   ---------------------------------------- 0.0/42.2 MB ? eta -:--:--\n","   ---------------------------------------- 0.2/42.2 MB 3.9 MB/s eta 0:00:11\n","   ---------------------------------------- 0.4/42.2 MB 3.7 MB/s eta 0:00:12\n","    --------------------------------------- 0.5/42.2 MB 3.7 MB/s eta 0:00:12\n","    --------------------------------------- 0.7/42.2 MB 3.9 MB/s eta 0:00:11\n","    --------------------------------------- 0.9/42.2 MB 3.7 MB/s eta 0:00:12\n","   - -------------------------------------- 1.1/42.2 MB 3.8 MB/s eta 0:00:11\n","   - -------------------------------------- 1.2/42.2 MB 3.7 MB/s eta 0:00:12\n","   - -------------------------------------- 1.4/42.2 MB 3.8 MB/s eta 0:00:11\n","   - -------------------------------------- 1.7/42.2 MB 3.8 MB/s eta 0:00:11\n","   - -------------------------------------- 1.8/42.2 MB 3.9 MB/s eta 0:00:11\n","   - -------------------------------------- 2.1/42.2 MB 3.9 MB/s eta 0:00:11\n","   -- ------------------------------------- 2.3/42.2 MB 4.0 MB/s eta 0:00:10\n","   -- ------------------------------------- 2.6/42.2 MB 4.0 MB/s eta 0:00:10\n","   -- ------------------------------------- 2.8/42.2 MB 4.2 MB/s eta 0:00:10\n","   -- ------------------------------------- 3.1/42.2 MB 4.3 MB/s eta 0:00:10\n","   --- ------------------------------------ 3.2/42.2 MB 4.2 MB/s eta 0:00:10\n","   --- ------------------------------------ 3.2/42.2 MB 4.0 MB/s eta 0:00:10\n","   --- ------------------------------------ 3.5/42.2 MB 4.0 MB/s eta 0:00:10\n","   --- ------------------------------------ 3.6/42.2 MB 4.0 MB/s eta 0:00:10\n","   --- ------------------------------------ 3.8/42.2 MB 4.0 MB/s eta 0:00:10\n","   --- ------------------------------------ 4.1/42.2 MB 4.1 MB/s eta 0:00:10\n","   ---- ----------------------------------- 4.3/42.2 MB 4.0 MB/s eta 0:00:10\n","   ---- ----------------------------------- 4.5/42.2 MB 4.1 MB/s eta 0:00:10\n","   ---- ----------------------------------- 4.6/42.2 MB 4.0 MB/s eta 0:00:10\n","   ---- ----------------------------------- 4.8/42.2 MB 4.0 MB/s eta 0:00:10\n","   ---- ----------------------------------- 5.0/42.2 MB 4.0 MB/s eta 0:00:10\n","   ---- ----------------------------------- 5.1/42.2 MB 4.0 MB/s eta 0:00:10\n","   ----- ---------------------------------- 5.3/42.2 MB 3.9 MB/s eta 0:00:10\n","   ----- ---------------------------------- 5.5/42.2 MB 3.9 MB/s eta 0:00:10\n","   ----- ---------------------------------- 5.7/42.2 MB 3.9 MB/s eta 0:00:10\n","   ----- ---------------------------------- 5.8/42.2 MB 3.9 MB/s eta 0:00:10\n","   ----- ---------------------------------- 6.0/42.2 MB 3.9 MB/s eta 0:00:10\n","   ----- ---------------------------------- 6.2/42.2 MB 3.9 MB/s eta 0:00:10\n","   ----- ---------------------------------- 6.3/42.2 MB 3.9 MB/s eta 0:00:10\n","   ------ --------------------------------- 6.5/42.2 MB 3.9 MB/s eta 0:00:10\n","   ------ --------------------------------- 6.7/42.2 MB 3.9 MB/s eta 0:00:10\n","   ------ --------------------------------- 6.9/42.2 MB 3.8 MB/s eta 0:00:10\n","   ------ --------------------------------- 7.0/42.2 MB 3.8 MB/s eta 0:00:10\n","   ------ --------------------------------- 7.1/42.2 MB 3.8 MB/s eta 0:00:10\n","   ------ --------------------------------- 7.3/42.2 MB 3.8 MB/s eta 0:00:10\n","   ------- -------------------------------- 7.5/42.2 MB 3.8 MB/s eta 0:00:10\n","   ------- -------------------------------- 7.7/42.2 MB 3.8 MB/s eta 0:00:10\n","   ------- -------------------------------- 7.9/42.2 MB 3.8 MB/s eta 0:00:10\n","   ------- -------------------------------- 8.0/42.2 MB 3.8 MB/s eta 0:00:09\n","   ------- -------------------------------- 8.2/42.2 MB 3.8 MB/s eta 0:00:09\n","   ------- -------------------------------- 8.3/42.2 MB 3.8 MB/s eta 0:00:09\n","   -------- ------------------------------- 8.5/42.2 MB 3.8 MB/s eta 0:00:09\n","   -------- ------------------------------- 8.7/42.2 MB 3.8 MB/s eta 0:00:09\n","   -------- ------------------------------- 8.8/42.2 MB 3.7 MB/s eta 0:00:09\n","   -------- ------------------------------- 9.0/42.2 MB 3.7 MB/s eta 0:00:09\n","   -------- ------------------------------- 9.1/42.2 MB 3.8 MB/s eta 0:00:09\n","   -------- ------------------------------- 9.3/42.2 MB 3.8 MB/s eta 0:00:09\n","   -------- ------------------------------- 9.5/42.2 MB 3.7 MB/s eta 0:00:09\n","   --------- ------------------------------ 9.6/42.2 MB 3.7 MB/s eta 0:00:09\n","   --------- ------------------------------ 9.8/42.2 MB 3.7 MB/s eta 0:00:09\n","   --------- ------------------------------ 10.0/42.2 MB 3.8 MB/s eta 0:00:09\n","   --------- ------------------------------ 10.2/42.2 MB 3.8 MB/s eta 0:00:09\n","   --------- ------------------------------ 10.3/42.2 MB 3.7 MB/s eta 0:00:09\n","   --------- ------------------------------ 10.5/42.2 MB 3.7 MB/s eta 0:00:09\n","   ---------- ----------------------------- 10.6/42.2 MB 3.7 MB/s eta 0:00:09\n","   ---------- ----------------------------- 10.8/42.2 MB 3.7 MB/s eta 0:00:09\n","   ---------- ----------------------------- 11.0/42.2 MB 3.7 MB/s eta 0:00:09\n","   ---------- ----------------------------- 11.2/42.2 MB 3.8 MB/s eta 0:00:09\n","   ---------- ----------------------------- 11.4/42.2 MB 3.7 MB/s eta 0:00:09\n","   ---------- ----------------------------- 11.5/42.2 MB 3.7 MB/s eta 0:00:09\n","   ----------- ---------------------------- 11.7/42.2 MB 3.7 MB/s eta 0:00:09\n","   ----------- ---------------------------- 11.9/42.2 MB 3.7 MB/s eta 0:00:09\n","   ----------- ---------------------------- 12.0/42.2 MB 3.7 MB/s eta 0:00:09\n","   ----------- ---------------------------- 12.2/42.2 MB 3.7 MB/s eta 0:00:09\n","   ----------- ---------------------------- 12.4/42.2 MB 3.7 MB/s eta 0:00:09\n","   ----------- ---------------------------- 12.5/42.2 MB 3.7 MB/s eta 0:00:09\n","   ------------ --------------------------- 12.7/42.2 MB 3.6 MB/s eta 0:00:09\n","   ------------ --------------------------- 12.8/42.2 MB 3.6 MB/s eta 0:00:09\n","   ------------ --------------------------- 13.0/42.2 MB 3.6 MB/s eta 0:00:09\n","   ------------ --------------------------- 13.3/42.2 MB 3.6 MB/s eta 0:00:09\n","   ------------ --------------------------- 13.4/42.2 MB 3.6 MB/s eta 0:00:09\n","   ------------ --------------------------- 13.7/42.2 MB 3.6 MB/s eta 0:00:08\n","   ------------ --------------------------- 13.7/42.2 MB 3.6 MB/s eta 0:00:08\n","   ------------- -------------------------- 13.9/42.2 MB 3.6 MB/s eta 0:00:08\n","   ------------- -------------------------- 14.1/42.2 MB 3.6 MB/s eta 0:00:08\n","   ------------- -------------------------- 14.3/42.2 MB 3.6 MB/s eta 0:00:08\n","   ------------- -------------------------- 14.5/42.2 MB 3.6 MB/s eta 0:00:08\n","   ------------- -------------------------- 14.7/42.2 MB 3.6 MB/s eta 0:00:08\n","   -------------- ------------------------- 15.0/42.2 MB 3.6 MB/s eta 0:00:08\n","   -------------- ------------------------- 15.1/42.2 MB 3.6 MB/s eta 0:00:08\n","   -------------- ------------------------- 15.2/42.2 MB 3.5 MB/s eta 0:00:08\n","   -------------- ------------------------- 15.4/42.2 MB 3.5 MB/s eta 0:00:08\n","   -------------- ------------------------- 15.6/42.2 MB 3.6 MB/s eta 0:00:08\n","   -------------- ------------------------- 15.7/42.2 MB 3.6 MB/s eta 0:00:08\n","   --------------- ------------------------ 15.9/42.2 MB 3.6 MB/s eta 0:00:08\n","   --------------- ------------------------ 16.1/42.2 MB 3.6 MB/s eta 0:00:08\n","   --------------- ------------------------ 16.2/42.2 MB 3.6 MB/s eta 0:00:08\n","   --------------- ------------------------ 16.3/42.2 MB 3.5 MB/s eta 0:00:08\n","   --------------- ------------------------ 16.5/42.2 MB 3.5 MB/s eta 0:00:08\n","   --------------- ------------------------ 16.7/42.2 MB 3.6 MB/s eta 0:00:08\n","   --------------- ------------------------ 16.8/42.2 MB 3.6 MB/s eta 0:00:08\n","   ---------------- ----------------------- 16.9/42.2 MB 3.5 MB/s eta 0:00:08\n","   ---------------- ----------------------- 17.2/42.2 MB 3.5 MB/s eta 0:00:08\n","   ---------------- ----------------------- 17.4/42.2 MB 3.6 MB/s eta 0:00:07\n","   ---------------- ----------------------- 17.6/42.2 MB 3.6 MB/s eta 0:00:07\n","   ---------------- ----------------------- 17.8/42.2 MB 3.6 MB/s eta 0:00:07\n","   ---------------- ----------------------- 17.9/42.2 MB 3.6 MB/s eta 0:00:07\n","   ----------------- ---------------------- 18.1/42.2 MB 3.6 MB/s eta 0:00:07\n","   ----------------- ---------------------- 18.3/42.2 MB 3.6 MB/s eta 0:00:07\n","   ----------------- ---------------------- 18.5/42.2 MB 3.6 MB/s eta 0:00:07\n","   ----------------- ---------------------- 18.6/42.2 MB 3.6 MB/s eta 0:00:07\n","   ----------------- ---------------------- 18.8/42.2 MB 3.6 MB/s eta 0:00:07\n","   ----------------- ---------------------- 18.9/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------ --------------------- 19.1/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------ --------------------- 19.2/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------ --------------------- 19.4/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------ --------------------- 19.5/42.2 MB 3.5 MB/s eta 0:00:07\n","   ------------------ --------------------- 19.8/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------ --------------------- 19.9/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------- -------------------- 20.1/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------- -------------------- 20.2/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------- -------------------- 20.4/42.2 MB 3.5 MB/s eta 0:00:07\n","   ------------------- -------------------- 20.7/42.2 MB 3.6 MB/s eta 0:00:07\n","   ------------------- -------------------- 20.8/42.2 MB 3.6 MB/s eta 0:00:06\n","   ------------------- -------------------- 21.0/42.2 MB 3.6 MB/s eta 0:00:06\n","   -------------------- ------------------- 21.1/42.2 MB 3.6 MB/s eta 0:00:06\n","   -------------------- ------------------- 21.3/42.2 MB 3.6 MB/s eta 0:00:06\n","   -------------------- ------------------- 21.3/42.2 MB 3.5 MB/s eta 0:00:06\n","   -------------------- ------------------- 21.3/42.2 MB 3.5 MB/s eta 0:00:06\n","   -------------------- ------------------- 21.3/42.2 MB 3.4 MB/s eta 0:00:07\n","   -------------------- ------------------- 21.3/42.2 MB 3.4 MB/s eta 0:00:07\n","   -------------------- ------------------- 21.4/42.2 MB 3.3 MB/s eta 0:00:07\n","   -------------------- ------------------- 21.6/42.2 MB 3.3 MB/s eta 0:00:07\n","   -------------------- ------------------- 21.8/42.2 MB 3.3 MB/s eta 0:00:07\n","   -------------------- ------------------- 22.0/42.2 MB 3.4 MB/s eta 0:00:07\n","   -------------------- ------------------- 22.2/42.2 MB 3.3 MB/s eta 0:00:07\n","   --------------------- ------------------ 22.4/42.2 MB 3.4 MB/s eta 0:00:06\n","   --------------------- ------------------ 22.6/42.2 MB 3.4 MB/s eta 0:00:06\n","   --------------------- ------------------ 22.8/42.2 MB 3.4 MB/s eta 0:00:06\n","   --------------------- ------------------ 23.0/42.2 MB 3.4 MB/s eta 0:00:06\n","   ---------------------- ----------------- 23.3/42.2 MB 3.4 MB/s eta 0:00:06\n","   ---------------------- ----------------- 23.6/42.2 MB 3.5 MB/s eta 0:00:06\n","   ---------------------- ----------------- 23.8/42.2 MB 3.5 MB/s eta 0:00:06\n","   ---------------------- ----------------- 24.2/42.2 MB 3.6 MB/s eta 0:00:06\n","   ----------------------- ---------------- 24.4/42.2 MB 3.6 MB/s eta 0:00:05\n","   ----------------------- ---------------- 24.5/42.2 MB 3.6 MB/s eta 0:00:05\n","   ----------------------- ---------------- 24.6/42.2 MB 3.6 MB/s eta 0:00:05\n","   ----------------------- ---------------- 24.8/42.2 MB 3.5 MB/s eta 0:00:05\n","   ----------------------- ---------------- 24.8/42.2 MB 3.5 MB/s eta 0:00:05\n","   ----------------------- ---------------- 25.1/42.2 MB 3.5 MB/s eta 0:00:05\n","   ------------------------ --------------- 25.4/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------ --------------- 25.7/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------ --------------- 25.8/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------ --------------- 26.0/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------ --------------- 26.2/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------ --------------- 26.3/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------- -------------- 26.5/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------- -------------- 26.7/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------- -------------- 26.9/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------- -------------- 27.1/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------- -------------- 27.2/42.2 MB 3.6 MB/s eta 0:00:05\n","   ------------------------- -------------- 27.4/42.2 MB 3.6 MB/s eta 0:00:05\n","   -------------------------- ------------- 27.6/42.2 MB 3.6 MB/s eta 0:00:05\n","   -------------------------- ------------- 27.8/42.2 MB 3.6 MB/s eta 0:00:05\n","   -------------------------- ------------- 27.9/42.2 MB 3.6 MB/s eta 0:00:05\n","   -------------------------- ------------- 28.1/42.2 MB 3.6 MB/s eta 0:00:04\n","   -------------------------- ------------- 28.3/42.2 MB 3.6 MB/s eta 0:00:04\n","   -------------------------- ------------- 28.4/42.2 MB 3.6 MB/s eta 0:00:04\n","   --------------------------- ------------ 28.6/42.2 MB 3.6 MB/s eta 0:00:04\n","   --------------------------- ------------ 28.8/42.2 MB 3.6 MB/s eta 0:00:04\n","   --------------------------- ------------ 29.0/42.2 MB 3.6 MB/s eta 0:00:04\n","   --------------------------- ------------ 29.2/42.2 MB 3.6 MB/s eta 0:00:04\n","   --------------------------- ------------ 29.3/42.2 MB 3.6 MB/s eta 0:00:04\n","   --------------------------- ------------ 29.5/42.2 MB 3.6 MB/s eta 0:00:04\n","   ---------------------------- ----------- 29.7/42.2 MB 3.6 MB/s eta 0:00:04\n","   ---------------------------- ----------- 29.8/42.2 MB 3.6 MB/s eta 0:00:04\n","   ---------------------------- ----------- 30.0/42.2 MB 3.6 MB/s eta 0:00:04\n","   ---------------------------- ----------- 30.1/42.2 MB 3.6 MB/s eta 0:00:04\n","   ---------------------------- ----------- 30.3/42.2 MB 3.5 MB/s eta 0:00:04\n","   ---------------------------- ----------- 30.5/42.2 MB 3.6 MB/s eta 0:00:04\n","   ---------------------------- ----------- 30.6/42.2 MB 3.6 MB/s eta 0:00:04\n","   ----------------------------- ---------- 30.8/42.2 MB 3.6 MB/s eta 0:00:04\n","   ----------------------------- ---------- 30.9/42.2 MB 3.6 MB/s eta 0:00:04\n","   ----------------------------- ---------- 31.1/42.2 MB 3.6 MB/s eta 0:00:04\n","   ----------------------------- ---------- 31.3/42.2 MB 3.6 MB/s eta 0:00:04\n","   ----------------------------- ---------- 31.5/42.2 MB 3.6 MB/s eta 0:00:04\n","   ------------------------------ --------- 31.7/42.2 MB 3.9 MB/s eta 0:00:03\n","   ------------------------------ --------- 31.9/42.2 MB 3.9 MB/s eta 0:00:03\n","   ------------------------------ --------- 32.0/42.2 MB 3.8 MB/s eta 0:00:03\n","   ------------------------------ --------- 32.3/42.2 MB 3.9 MB/s eta 0:00:03\n","   ------------------------------ --------- 32.4/42.2 MB 3.8 MB/s eta 0:00:03\n","   ------------------------------ --------- 32.6/42.2 MB 3.8 MB/s eta 0:00:03\n","   ------------------------------- -------- 32.8/42.2 MB 3.8 MB/s eta 0:00:03\n","   ------------------------------- -------- 32.9/42.2 MB 3.8 MB/s eta 0:00:03\n","   ------------------------------- -------- 33.1/42.2 MB 3.8 MB/s eta 0:00:03\n","   ------------------------------- -------- 33.2/42.2 MB 3.7 MB/s eta 0:00:03\n","   ------------------------------- -------- 33.4/42.2 MB 3.7 MB/s eta 0:00:03\n","   ------------------------------- -------- 33.7/42.2 MB 3.7 MB/s eta 0:00:03\n","   ------------------------------- -------- 33.8/42.2 MB 3.7 MB/s eta 0:00:03\n","   -------------------------------- ------- 34.0/42.2 MB 3.6 MB/s eta 0:00:03\n","   -------------------------------- ------- 34.1/42.2 MB 3.6 MB/s eta 0:00:03\n","   -------------------------------- ------- 34.3/42.2 MB 3.6 MB/s eta 0:00:03\n","   -------------------------------- ------- 34.4/42.2 MB 3.6 MB/s eta 0:00:03\n","   -------------------------------- ------- 34.6/42.2 MB 3.6 MB/s eta 0:00:03\n","   -------------------------------- ------- 34.8/42.2 MB 3.6 MB/s eta 0:00:03\n","   --------------------------------- ------ 35.0/42.2 MB 3.6 MB/s eta 0:00:03\n","   --------------------------------- ------ 35.2/42.2 MB 3.6 MB/s eta 0:00:02\n","   --------------------------------- ------ 35.3/42.2 MB 3.6 MB/s eta 0:00:02\n","   --------------------------------- ------ 35.5/42.2 MB 3.6 MB/s eta 0:00:02\n","   --------------------------------- ------ 35.7/42.2 MB 3.6 MB/s eta 0:00:02\n","   ---------------------------------- ----- 35.9/42.2 MB 3.6 MB/s eta 0:00:02\n","   ---------------------------------- ----- 36.0/42.2 MB 3.6 MB/s eta 0:00:02\n","   ---------------------------------- ----- 36.2/42.2 MB 3.6 MB/s eta 0:00:02\n","   ---------------------------------- ----- 36.4/42.2 MB 3.6 MB/s eta 0:00:02\n","   ---------------------------------- ----- 36.5/42.2 MB 3.6 MB/s eta 0:00:02\n","   ---------------------------------- ----- 36.6/42.2 MB 3.5 MB/s eta 0:00:02\n","   ---------------------------------- ----- 36.8/42.2 MB 3.5 MB/s eta 0:00:02\n","   ----------------------------------- ---- 37.1/42.2 MB 3.6 MB/s eta 0:00:02\n","   ----------------------------------- ---- 37.2/42.2 MB 3.5 MB/s eta 0:00:02\n","   ----------------------------------- ---- 37.3/42.2 MB 3.6 MB/s eta 0:00:02\n","   ----------------------------------- ---- 37.6/42.2 MB 3.6 MB/s eta 0:00:02\n","   ----------------------------------- ---- 37.8/42.2 MB 3.5 MB/s eta 0:00:02\n","   ----------------------------------- ---- 37.9/42.2 MB 3.6 MB/s eta 0:00:02\n","   ------------------------------------ --- 38.1/42.2 MB 3.6 MB/s eta 0:00:02\n","   ------------------------------------ --- 38.3/42.2 MB 3.6 MB/s eta 0:00:02\n","   ------------------------------------ --- 38.5/42.2 MB 3.5 MB/s eta 0:00:02\n","   ------------------------------------ --- 38.7/42.2 MB 3.6 MB/s eta 0:00:01\n","   ------------------------------------ --- 38.8/42.2 MB 3.6 MB/s eta 0:00:01\n","   ------------------------------------ --- 39.0/42.2 MB 3.5 MB/s eta 0:00:01\n","   ------------------------------------- -- 39.1/42.2 MB 3.5 MB/s eta 0:00:01\n","   ------------------------------------- -- 39.4/42.2 MB 3.6 MB/s eta 0:00:01\n","   ------------------------------------- -- 39.5/42.2 MB 3.5 MB/s eta 0:00:01\n","   ------------------------------------- -- 39.6/42.2 MB 3.6 MB/s eta 0:00:01\n","   ------------------------------------- -- 39.8/42.2 MB 3.6 MB/s eta 0:00:01\n","   ------------------------------------- -- 40.0/42.2 MB 3.5 MB/s eta 0:00:01\n","   -------------------------------------- - 40.2/42.2 MB 3.6 MB/s eta 0:00:01\n","   -------------------------------------- - 40.3/42.2 MB 3.6 MB/s eta 0:00:01\n","   -------------------------------------- - 40.5/42.2 MB 3.6 MB/s eta 0:00:01\n","   -------------------------------------- - 40.6/42.2 MB 3.6 MB/s eta 0:00:01\n","   -------------------------------------- - 40.8/42.2 MB 3.6 MB/s eta 0:00:01\n","   -------------------------------------- - 41.0/42.2 MB 3.6 MB/s eta 0:00:01\n","   -------------------------------------- - 41.1/42.2 MB 3.6 MB/s eta 0:00:01\n","   ---------------------------------------  41.4/42.2 MB 3.6 MB/s eta 0:00:01\n","   ---------------------------------------  41.5/42.2 MB 3.6 MB/s eta 0:00:01\n","   ---------------------------------------  41.7/42.2 MB 3.5 MB/s eta 0:00:01\n","   ---------------------------------------  41.9/42.2 MB 3.5 MB/s eta 0:00:01\n","   ---------------------------------------  42.0/42.2 MB 3.5 MB/s eta 0:00:01\n","   ---------------------------------------  42.2/42.2 MB 3.6 MB/s eta 0:00:01\n","   ---------------------------------------  42.2/42.2 MB 3.5 MB/s eta 0:00:01\n","   ---------------------------------------  42.2/42.2 MB 3.5 MB/s eta 0:00:01\n","   ---------------------------------------  42.2/42.2 MB 3.5 MB/s eta 0:00:01\n","   ---------------------------------------- 42.2/42.2 MB 3.4 MB/s eta 0:00:00\n","Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n","Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n","Successfully installed joblib-1.4.2 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.5.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install transformers torch pandas scikit-learn datasets torch accelerate evaluate"]},{"cell_type":"markdown","id":"0063af1e","metadata":{},"source":["Load the CodeSearchNet dataset"]},{"cell_type":"code","execution_count":null,"id":"d97faf80-7804-488b-a0a7-680aa8f5509c","metadata":{"id":"d97faf80-7804-488b-a0a7-680aa8f5509c"},"outputs":[],"source":["from datasets import load_dataset, load_metric\n","\n","# Load the CodeSearchNet dataset\n","dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"javascript\")\n","train_data = dataset[\"train\"]\n","val_data = dataset[\"validation\"]\n","test_data = dataset[\"test\"]\n","\n","\n","# Sample a subset of the data for quick training/testing\n","train_data = train_data.shuffle(seed=42).select(range(20000))\n","val_data = val_data.shuffle(seed=42).select(range(1500))"]},{"cell_type":"markdown","id":"8cb116e5","metadata":{},"source":["Initialize the tokenizer and model"]},{"cell_type":"code","execution_count":7,"id":"fde3b389","metadata":{},"outputs":[],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')"]},{"cell_type":"code","execution_count":null,"id":"7ebcae84","metadata":{},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"id":"a3433d05","metadata":{},"outputs":[],"source":["# Add padding token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Tokenize the dataset\n","def tokenize_function(examples):\n","    code = examples['code']\n","    docstring = examples['docstring']\n","    inputs = tokenizer(code, max_length=512, truncation=True, padding=\"max_length\")\n","    labels = tokenizer(docstring, max_length=128, truncation=True, padding=\"max_length\")\n","    inputs['labels'] = labels['input_ids']\n","    return inputs\n","\n","# Tokenize the training and validation datasets\n","tokenized_train_data = train_data.map(tokenize_function, batched=True, remove_columns=['repo', 'path', 'func_name', 'original_string', 'code_tokens', 'docstring_tokens'])\n","tokenized_val_data = val_data.map(tokenize_function, batched=True, remove_columns=['repo', 'path', 'func_name', 'original_string', 'code_tokens', 'docstring_tokens'])\n"]},{"cell_type":"code","execution_count":1,"id":"7f4a48b1-a1e2-4128-ab7a-3e4d6dab7fa2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":33221,"status":"error","timestamp":1719548358711,"user":{"displayName":"NNM22IS099 NAMRATHA M","userId":"08451226351542548896"},"user_tz":-330},"id":"7f4a48b1-a1e2-4128-ab7a-3e4d6dab7fa2","outputId":"aca59ec6-5f7e-4296-e9ab-bf19a0671250"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8564d28a1446>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForLanguageModeling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","from datasets import load_dataset, load_metric\n","import pandas as pd\n","\n","\n","\n","# Create a data collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","# Set up training arguments\n","training_args = TrainingArguments(\n","    output_dir='./GPT2_20000',\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=20,\n","    per_device_eval_batch_size=4,\n","    #warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./GPT2_20000',\n","    logging_steps=10,\n","    save_total_limit=2,\n","    #load_best_model_at_end=True,\n","    #metric_for_best_model=True,\n","    #greater_is_better=False,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    # resume_from_checkpoint=True,\n",")\n","\n","# Move model to the correct device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# # Load the ROUGE metric\n","# rouge = load_metric(\"rouge\")\n","\n","# # Function to compute the metric\n","# def compute_metrics(pred):\n","#     labels_ids = pred.label_ids\n","#     pred_ids = pred.predictions\n","\n","#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","#     labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","#     # Compute the metric\n","#     rouge_output = rouge.compute(predictions=pred_str, references=labels_str)\n","#     return {\n","#         \"rouge1\": rouge_output[\"rouge1\"].mid.fmeasure,\n","#         \"rouge2\": rouge_output[\"rouge2\"].mid.fmeasure,\n","#         \"rougeL\": rouge_output[\"rougeL\"].mid.fmeasure,\n","#     }\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_data,\n","    eval_dataset=tokenized_val_data,\n","    data_collator=data_collator,\n","    # compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","# trainer.train(resume_from_checkpoint=True)\n","trainer.train()\n","\n","# Evaluate the model\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results: {eval_results}\")\n","\n","# Save the model and tokenizer after training\n","model.save_pretrained('./GPT2_20000')\n","tokenizer.save_pretrained('./GPT2_20000')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"40940c18-4771-4168-a533-26b1bb09915b","metadata":{"id":"40940c18-4771-4168-a533-26b1bb09915b"},"outputs":[],"source":["# import torch\n","# from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","# from datasets import load_dataset, load_metric\n","# import pandas as pd\n","\n","# # Function to generate summaries\n","# def generate_summary(code_snippet):\n","#     inputs = tokenizer(code_snippet, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n","#     outputs = model.generate(inputs.input_ids, max_length=128, num_beams=4, early_stopping=True)\n","#     summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","#     return summary\n","\n","# # Example usage\n","# code_snippet = \"def add(a, b):\\n    return a + b\"\n","# summary = generate_summary(code_snippet)\n","# print(f\"Code:\\n{code_snippet}\\nSummary:\\n{summary}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4e61cce5-0557-4e57-beef-656127f520c1","metadata":{"id":"4e61cce5-0557-4e57-beef-656127f520c1","outputId":"17c81574-e2f0-4718-e52b-3053a36270ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","torch.cuda.empty_cache()\n","# Determine the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the saved model and tokenizer\n","model_path = './GPT2'  # Change this to your actual model path if different\n","tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","model = GPT2LMHeadModel.from_pretrained(model_path)\n","model.to(device)\n","\n","# Add padding token if it's not there\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","# Function to generate a summary from a code snippet\n","def generate_summary(code_snippet, model, tokenizer):\n","    input_text = f\"Summarize the following code: {code_snippet}\\nSummary:\"\n","    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n","    inputs = inputs.to(device)\n","\n","    summary_ids = model.generate(\n","        inputs.input_ids,\n","        max_length=150,\n","        num_beams=4,\n","        early_stopping=True,\n","        no_repeat_ngram_size=2\n","    )\n","\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summary\n","\n","# Example usage\n","code_snippet = '''\n","let a=parseInt(4);\n","let b=parseInt(3);\n","let res=a+b;\n","return res;\n","'''\n","\n","summary = generate_summary(code_snippet, model, tokenizer)\n","print(\"Generated Summary:\")\n","print(summary)\n"]},{"cell_type":"code","execution_count":null,"id":"c8e2d6d4-df77-47e8-8f50-7817d622c8b4","metadata":{"id":"c8e2d6d4-df77-47e8-8f50-7817d622c8b4"},"outputs":[],"source":["# from datasets import load_dataset, load_metric\n","# import torch\n","# from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","\n","# model = GPT2LMHeadModel.from_pretrained('gpt2')\n","# # Initialize the tokenizer and model\n","# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","# model = GPT2LMHeadModel.from_pretrained('gpt2')\n","# # Create a data collator\n","# data_collator = DataCollatorForLanguageModeling(\n","#     tokenizer=tokenizer,\n","#     mlm=False,\n","# )\n","\n","# # Set up training arguments\n","# training_args = TrainingArguments(\n","#     output_dir='./GPT2',\n","#     overwrite_output_dir=True,\n","#     num_train_epochs=3,\n","#     per_device_train_batch_size=4,\n","#     per_device_eval_batch_size=4,\n","#     warmup_steps=500,\n","#     weight_decay=0.01,\n","#     logging_dir='./GPT2',\n","#     logging_steps=10,\n","#     evaluation_strategy=\"epoch\",\n","# )\n","\n","# # Initialize the Trainer\n","# trainer = Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=tokenized_train_data,\n","#     eval_dataset=tokenized_val_data,\n","#     data_collator=data_collator,\n","# )\n","\n","\n","# # Determine the device\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# # Load the ROUGE metric\n","# rouge = load_metric(\"rouge\")\n","\n","# # Function to compute the metric\n","# def compute_metrics(pred):\n","#     labels_ids = pred.label_ids\n","#     pred_ids = pred.predictions\n","\n","#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","#     labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","#     # Compute the metric\n","#     rouge_output = rouge.compute(predictions=pred_str, references=labels_str)\n","#     return {\n","#         \"rouge1\": rouge_output[\"rouge1\"].mid.fmeasure,\n","#         \"rouge2\": rouge_output[\"rouge2\"].mid.fmeasure,\n","#         \"rougeL\": rouge_output[\"rougeL\"].mid.fmeasure,\n","#     }\n","\n","# # Add the compute_metrics function to the trainer\n","# trainer.compute_metrics = compute_metrics\n","\n","# # Re-evaluate the model with the compute_metrics function\n","# eval_results = trainer.evaluate()\n","# print(f\"Evaluation Results with ROUGE: {eval_results}\")"]},{"cell_type":"code","execution_count":null,"id":"49ffa261-8e51-4384-8011-a834583f357e","metadata":{"id":"49ffa261-8e51-4384-8011-a834583f357e"},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":5}
=======
{"cells":[{"cell_type":"code","execution_count":null,"id":"3b3578df-f132-4673-9f93-6f9df056952c","metadata":{"id":"3b3578df-f132-4673-9f93-6f9df056952c","outputId":"287e92ee-d6aa-4bc1-c60b-e4936b97fe5f","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (4.41.2)\n","Requirement already satisfied: torch in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (2.3.1)\n","Requirement already satisfied: pandas in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (1.5.0)\n","Requirement already satisfied: datasets in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (2.20.0)\n","Requirement already satisfied: accelerate in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (0.31.0)\n","Requirement already satisfied: evaluate in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (0.4.2)\n","Requirement already satisfied: filelock in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (3.15.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (2.0.0)\n","Requirement already satisfied: packaging>=20.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (2024.5.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from torch) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.40)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/nm788186/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: scipy>=1.6.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: xxhash in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: psutil in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /home/nm788186/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/nm788186/anaconda3/envs/myenv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install transformers torch pandas scikit-learn datasets torch accelerate evaluate"]},{"cell_type":"code","execution_count":null,"id":"5846be66","metadata":{},"outputs":[],"source":["%pip install -q gradio"]},{"cell_type":"code","execution_count":null,"id":"d97faf80-7804-488b-a0a7-680aa8f5509c","metadata":{"id":"d97faf80-7804-488b-a0a7-680aa8f5509c"},"outputs":[],"source":["from datasets import load_dataset, load_metric\n","\n","# Load the CodeSearchNet dataset\n","dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"javascript\")\n","train_data = dataset[\"train\"]\n","val_data = dataset[\"validation\"]\n","test_data = dataset[\"test\"]\n","\n","\n","# Sample a subset of the data for quick training/testing\n","train_data = train_data.shuffle(seed=42).select(range(20000))\n","val_data = val_data.shuffle(seed=42).select(range(1500))"]},{"cell_type":"code","execution_count":1,"id":"7f4a48b1-a1e2-4128-ab7a-3e4d6dab7fa2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":33221,"status":"error","timestamp":1719548358711,"user":{"displayName":"NNM22IS099 NAMRATHA M","userId":"08451226351542548896"},"user_tz":-330},"id":"7f4a48b1-a1e2-4128-ab7a-3e4d6dab7fa2","outputId":"aca59ec6-5f7e-4296-e9ab-bf19a0671250"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8564d28a1446>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForLanguageModeling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","from datasets import load_dataset, load_metric\n","import pandas as pd\n","\n","torch.cuda.empty_cache()\n","\n","# # Load the CodeSearchNet dataset\n","# dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"javascript\")\n","# train_data = dataset[\"train\"]\n","# val_data = dataset[\"validation\"]\n","# test_data = dataset[\"test\"]\n","\n","# # Sample a subset of the data for quick training/testing\n","# train_data = train_data.shuffle(seed=42).select(range(2000))\n","# val_data = val_data.shuffle(seed=42).select(range(300))\n","\n","# Initialize the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Add padding token\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Tokenize the dataset\n","def tokenize_function(examples):\n","    code = examples['code']\n","    docstring = examples['docstring']\n","    inputs = tokenizer(code, max_length=512, truncation=True, padding=\"max_length\")\n","    labels = tokenizer(docstring, max_length=128, truncation=True, padding=\"max_length\")\n","    inputs['labels'] = labels['input_ids']\n","    return inputs\n","\n","# Tokenize the training and validation datasets\n","tokenized_train_data = train_data.map(tokenize_function, batched=True, remove_columns=['repo', 'path', 'func_name', 'original_string', 'code_tokens', 'docstring_tokens'])\n","tokenized_val_data = val_data.map(tokenize_function, batched=True, remove_columns=['repo', 'path', 'func_name', 'original_string', 'code_tokens', 'docstring_tokens'])\n","\n","# Create a data collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","# Set up training arguments\n","training_args = TrainingArguments(\n","    output_dir='./GPT2_20000',\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=20,\n","    per_device_eval_batch_size=4,\n","    #warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./GPT2_20000',\n","    logging_steps=10,\n","    save_total_limit=2,\n","    #load_best_model_at_end=True,\n","    #metric_for_best_model=True,\n","    #greater_is_better=False,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    # resume_from_checkpoint=True,\n",")\n","\n","# Move model to the correct device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# # Load the ROUGE metric\n","# rouge = load_metric(\"rouge\")\n","\n","# # Function to compute the metric\n","# def compute_metrics(pred):\n","#     labels_ids = pred.label_ids\n","#     pred_ids = pred.predictions\n","\n","#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","#     labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","#     # Compute the metric\n","#     rouge_output = rouge.compute(predictions=pred_str, references=labels_str)\n","#     return {\n","#         \"rouge1\": rouge_output[\"rouge1\"].mid.fmeasure,\n","#         \"rouge2\": rouge_output[\"rouge2\"].mid.fmeasure,\n","#         \"rougeL\": rouge_output[\"rougeL\"].mid.fmeasure,\n","#     }\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_data,\n","    eval_dataset=tokenized_val_data,\n","    data_collator=data_collator,\n","    # compute_metrics=compute_metrics\n",")\n","\n","# Train the model\n","# trainer.train(resume_from_checkpoint=True)\n","trainer.train()\n","\n","# Evaluate the model\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results: {eval_results}\")\n","\n","# Save the model and tokenizer after training\n","model.save_pretrained('./GPT2_20000')\n","tokenizer.save_pretrained('./GPT2_20000')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"612551c8-5c4a-4662-8e94-5bce3a8b9a1d","metadata":{"id":"612551c8-5c4a-4662-8e94-5bce3a8b9a1d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"40940c18-4771-4168-a533-26b1bb09915b","metadata":{"id":"40940c18-4771-4168-a533-26b1bb09915b"},"outputs":[],"source":["# import torch\n","# from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","# from datasets import load_dataset, load_metric\n","# import pandas as pd\n","\n","# # Function to generate summaries\n","# def generate_summary(code_snippet):\n","#     inputs = tokenizer(code_snippet, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n","#     outputs = model.generate(inputs.input_ids, max_length=128, num_beams=4, early_stopping=True)\n","#     summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","#     return summary\n","\n","# # Example usage\n","# code_snippet = \"def add(a, b):\\n    return a + b\"\n","# summary = generate_summary(code_snippet)\n","# print(f\"Code:\\n{code_snippet}\\nSummary:\\n{summary}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4e61cce5-0557-4e57-beef-656127f520c1","metadata":{"id":"4e61cce5-0557-4e57-beef-656127f520c1","outputId":"17c81574-e2f0-4718-e52b-3053a36270ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","torch.cuda.empty_cache()\n","# Determine the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the saved model and tokenizer\n","model_path = './GPT2'  # Change this to your actual model path if different\n","tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","model = GPT2LMHeadModel.from_pretrained(model_path)\n","model.to(device)\n","\n","# Add padding token if it's not there\n","if tokenizer.pad_token is None:\n","    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","# Function to generate a summary from a code snippet\n","def generate_summary(code_snippet, model, tokenizer):\n","    input_text = f\"Summarize the following code: {code_snippet}\\nSummary:\"\n","    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n","    inputs = inputs.to(device)\n","\n","    summary_ids = model.generate(\n","        inputs.input_ids,\n","        max_length=150,\n","        num_beams=4,\n","        early_stopping=True,\n","        no_repeat_ngram_size=2\n","    )\n","\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summary\n","\n","# Example usage\n","code_snippet = '''\n","let a=parseInt(4);\n","let b=parseInt(3);\n","let res=a+b;\n","return res;\n","'''\n","\n","summary = generate_summary(code_snippet, model, tokenizer)\n","print(\"Generated Summary:\")\n","print(summary)\n"]},{"cell_type":"code","execution_count":null,"id":"c8e2d6d4-df77-47e8-8f50-7817d622c8b4","metadata":{"id":"c8e2d6d4-df77-47e8-8f50-7817d622c8b4"},"outputs":[],"source":["# from datasets import load_dataset, load_metric\n","# import torch\n","# from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","\n","# model = GPT2LMHeadModel.from_pretrained('gpt2')\n","# # Initialize the tokenizer and model\n","# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","# model = GPT2LMHeadModel.from_pretrained('gpt2')\n","# # Create a data collator\n","# data_collator = DataCollatorForLanguageModeling(\n","#     tokenizer=tokenizer,\n","#     mlm=False,\n","# )\n","\n","# # Set up training arguments\n","# training_args = TrainingArguments(\n","#     output_dir='./GPT2',\n","#     overwrite_output_dir=True,\n","#     num_train_epochs=3,\n","#     per_device_train_batch_size=4,\n","#     per_device_eval_batch_size=4,\n","#     warmup_steps=500,\n","#     weight_decay=0.01,\n","#     logging_dir='./GPT2',\n","#     logging_steps=10,\n","#     evaluation_strategy=\"epoch\",\n","# )\n","\n","# # Initialize the Trainer\n","# trainer = Trainer(\n","#     model=model,\n","#     args=training_args,\n","#     train_dataset=tokenized_train_data,\n","#     eval_dataset=tokenized_val_data,\n","#     data_collator=data_collator,\n","# )\n","\n","\n","# # Determine the device\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# # Load the ROUGE metric\n","# rouge = load_metric(\"rouge\")\n","\n","# # Function to compute the metric\n","# def compute_metrics(pred):\n","#     labels_ids = pred.label_ids\n","#     pred_ids = pred.predictions\n","\n","#     pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","#     labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","#     # Compute the metric\n","#     rouge_output = rouge.compute(predictions=pred_str, references=labels_str)\n","#     return {\n","#         \"rouge1\": rouge_output[\"rouge1\"].mid.fmeasure,\n","#         \"rouge2\": rouge_output[\"rouge2\"].mid.fmeasure,\n","#         \"rougeL\": rouge_output[\"rougeL\"].mid.fmeasure,\n","#     }\n","\n","# # Add the compute_metrics function to the trainer\n","# trainer.compute_metrics = compute_metrics\n","\n","# # Re-evaluate the model with the compute_metrics function\n","# eval_results = trainer.evaluate()\n","# print(f\"Evaluation Results with ROUGE: {eval_results}\")"]},{"cell_type":"code","execution_count":null,"id":"77355deb-3d8d-4ec8-a99e-2f4fed1e48f9","metadata":{"id":"77355deb-3d8d-4ec8-a99e-2f4fed1e48f9","outputId":"cae6d804-9075-4a63-a243-90f9ff33a568"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_3829775/1286409954.py:56: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  rouge = load_metric(\"rouge\")\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [75/75 00:35]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"int() argument must be a string, a bytes-like object or a real number, not 'list'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m trainer\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;241m=\u001b[39m compute_metrics\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Re-evaluate the model with the compute_metrics function\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Results with ROUGE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py:3854\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3850\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3851\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3852\u001b[0m         )\n\u001b[1;32m   3853\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3854\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3856\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n","Cell \u001b[0;32mIn[6], line 63\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m     60\u001b[0m labels_ids \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mlabel_ids\n\u001b[1;32m     61\u001b[0m pred_ids \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[0;32m---> 63\u001b[0m pred_str \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m labels_str \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(labels_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Compute the metric\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3796\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3772\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3773\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3774\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3777\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3780\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3781\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3794\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3795\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3798\u001b[0m             seq,\n\u001b[1;32m   3799\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3800\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3801\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3802\u001b[0m         )\n\u001b[1;32m   3803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3804\u001b[0m     ]\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3797\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3772\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3773\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3774\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3777\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3778\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3779\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3780\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3781\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3794\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3795\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3797\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3801\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3802\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3804\u001b[0m     ]\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3836\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3833\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3834\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3840\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3841\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils.py:1001\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    993\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    998\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1001\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1003\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1004\u001b[0m     }\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils.py:976\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    974\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 976\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"]}],"source":["from datasets import load_dataset, load_metric\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","torch.cuda.empty_cache()\n","\n","# Initialize the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# Add a padding token to the tokenizer\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Resize model embeddings to match the tokenizer\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Create a data collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","# Set up training arguments\n","training_args = TrainingArguments(\n","    output_dir='./GPT2',\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=4, # Reduce batch size\n","    gradient_accumulation_steps=2,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./GPT2',\n","    logging_steps=10,\n","    eval_strategy=\"epoch\",\n","    fp16=True,\n",")\n","\n","\n","# Assume tokenized_train_data and tokenized_val_data are already defined\n","# If not, you need to load and preprocess your dataset\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_data,\n","    eval_dataset=tokenized_val_data,\n","    data_collator=data_collator,\n",")\n","\n","# Determine the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Add the compute_metrics function to the trainer\n","trainer.compute_metrics = compute_metrics\n","\n","# Re-evaluate the model with the compute_metrics function\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results with ROUGE: {eval_results}\")\n"]},{"cell_type":"code","execution_count":null,"id":"d8f6f27e-3706-434c-8610-f73211d5d313","metadata":{"id":"d8f6f27e-3706-434c-8610-f73211d5d313","outputId":"2f7833ac-9274-4a48-c825-1145e966dacd"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='36' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [36/75 00:07 < 00:08, 4.81 it/s]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 14.19 GiB. GPU ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m trainer\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;241m=\u001b[39m compute_metrics\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Re-evaluate the model with the compute_metrics function\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation Results with ROUGE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer.py:3779\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3777\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   3778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 3779\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3781\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:326\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:140\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    143\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    144\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:99\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     96\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m    102\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.19 GiB. GPU "]}],"source":["from datasets import load_dataset, load_metric\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","torch.cuda.empty_cache()\n","\n","# Initialize the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","# Add a padding token to the tokenizer\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\n","\n","# Resize model embeddings to match the tokenizer\n","model.resize_token_embeddings(len(tokenizer))\n","\n","# Create a data collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,\n",")\n","\n","# Set up training arguments\n","training_args = TrainingArguments(\n","    output_dir='./GPT2',\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=4, # Reduce batch size\n","    gradient_accumulation_steps=2,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./GPT2',\n","    logging_steps=10,\n","    eval_strategy=\"epoch\",\n","    fp16=True,\n",")\n","\n","# Placeholder for tokenized datasets\n","# These should be defined appropriately as per your dataset\n","# tokenized_train_data = ...\n","# tokenized_val_data = ...\n","\n","# Tokenize the training and validation datasets\n","tokenized_train_data = train_data.map(tokenize_function, batched=True, remove_columns=['repo', 'path', 'func_name', 'original_string', 'code_tokens', 'docstring_tokens'])\n","tokenized_val_data = val_data.map(tokenize_function, batched=True, remove_columns=['repo', 'path', 'func_name', 'original_string', 'code_tokens', 'docstring_tokens'])\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train_data,\n","    eval_dataset=tokenized_val_data,\n","    data_collator=data_collator,\n",")\n","\n","# Determine the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the ROUGE metric\n","rouge = load_metric(\"rouge\")\n","\n","# Function to compute the metric\n","def compute_metrics(pred):\n","    labels_ids = pred.label_ids\n","    pred_ids = pred.predictions\n","\n","    # Ensure the predictions are lists of integers\n","    if isinstance(pred_ids, torch.Tensor):\n","        pred_ids = pred_ids.tolist()\n","    if isinstance(labels_ids, torch.Tensor):\n","        labels_ids = labels_ids.tolist()\n","\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n","\n","    # Compute the metric\n","    rouge_output = rouge.compute(predictions=pred_str, references=labels_str)\n","    return {\n","        \"rouge1\": rouge_output[\"rouge1\"].mid.fmeasure,\n","        \"rouge2\": rouge_output[\"rouge2\"].mid.fmeasure,\n","        \"rougeL\": rouge_output[\"rougeL\"].mid.fmeasure,\n","    }\n","\n","# Add the compute_metrics function to the trainer\n","trainer.compute_metrics = compute_metrics\n","\n","# Re-evaluate the model with the compute_metrics function\n","eval_results = trainer.evaluate()\n","print(f\"Evaluation Results with ROUGE: {eval_results}\")\n"]},{"cell_type":"code","execution_count":null,"id":"49ffa261-8e51-4384-8011-a834583f357e","metadata":{"id":"49ffa261-8e51-4384-8011-a834583f357e"},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"id":"8e1b626d","metadata":{},"outputs":[],"source":["import gradio as gr\n","output_text=gr.Textbox()\n","demo = gr.Interface(fn=generate_summary,\n","                   inputs=\"textbox\",\n","                   outputs=output_text,\n","                   title=\"Automatic Code Summarizer for Javascript\",\n","                   description=\"This app can summarize your javascript code snippets in natural language\")\n","demo.launch()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":5}
>>>>>>> 4095989d8bc54915228183e826b37d1bfb1a9e74
